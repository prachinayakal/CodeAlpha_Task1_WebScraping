# 🌐 Task 1: Web Scraping
Utilized Python with BeautifulSoup to perform web scraping from publicly available web sources. The collected data was structured and stored for use in subsequent data analysis tasks.       

## ✅ Objective  
To extract structured data from websites using Python libraries or automated tools, enabling the creation of custom datasets for further analysis.

## 🛠️ Tools & Technologies  
- Python  
- BeautifulSoup  
- Requests  
- Scrapy (for large-scale crawling)  
- Selenium (for dynamic websites)  
- Optional GUI tools: Octoparse / ParseHub (for no-code users)

## 📌 Key Deliverables  
- Web scraping script or automation project  
- Extracted and cleaned dataset (CSV/JSON format)  
- Documentation of the scraping process  
- Handling of HTML structure, tags, and website navigation logic

## 🔍 Highlights  
- Identified target websites and relevant data points  
- Parsed HTML content to extract required information  
- Handled pagination, dynamic content, and HTML tags  
- Stored scraped data in a structured format for further analysis  
- Respected website terms of service and included delay to avoid overloading servers

## 📁 Files Included  
- `web_scraping_script.py` – Python script for scraping  
- `scraped_data.csv` – Collected dataset  
- `README.md` – Project documentation  
- `requirements.txt` – Required Python packages

## ⚠️ Ethics and Legality  
- Always check the website’s `robots.txt` file before scraping  
- Do not scrape personal, copyrighted, or sensitive content  
- Use scraped data responsibly and for educational/research purposes only

## 🧠 Outcome  
This task demonstrates the use of automated methods to gather data directly from the web, creating custom datasets for real-world data analysis and decision-making.
